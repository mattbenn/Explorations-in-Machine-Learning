{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Overfit Tree Models\n",
    "\n",
    "By default, you scikitlearn tree models will grow until every node is pure.  To explore this, you are to build different models using the `max_depth` parameter and determine when the tree begins to overfit the data.  For depths from `max_depth = 1` until the tree is completed, keep track of the accuracy on training vs. test data and generate a plot with depths as the horizontal axis and accuracy as the vertical axis for train and test data.  \n",
    "\n",
    "Repeat this process with different splits of the data to determine at what depth the tree begins to overfit.  Share your results with your peers and discuss your approach to generating the visualization.  What are the consequences of this overfitting for your approach to building Decision Trees?   We provide a small dataset with health data where your goal is to predict whether or not the individuals survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Whickham.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy as a function of train-test-split\n",
    "Now, test to see how altering the relative size of the train-test split alters things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universal_random_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['smoker', 'age']]\n",
    "X['smoker'] = X['smoker'].map({'Yes': 1, 'No': 0})\n",
    "y = data['outcome']\n",
    "stratify_col = X['smoker'].astype(str) + \"_\" + y.astype(str)\n",
    "stratify_col.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = np.linspace(.1, .9, 9)\n",
    "max_depths = list(range(1, 16)) # iterate over max depth\n",
    "\n",
    "accuracy_dict = {}\n",
    "\n",
    "for temp_size in train_size:\n",
    "    accuracy_dict[temp_size] = {'train_score': [], 'test_score': []} # this time, instantiate train_score and test_score as empty lists onto which I will append accuracy values\n",
    "    temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, train_size = temp_size, random_state = universal_random_seed, stratify = stratify_col)\n",
    "   \n",
    "    for temp_depth in max_depths:\n",
    "        temp_tree = DecisionTreeClassifier(max_depth = temp_depth).fit(temp_X_train, temp_y_train)\n",
    "        \n",
    "        accuracy_dict[temp_size]['train_score'].append(temp_tree.score(temp_X_train, temp_y_train))\n",
    "        accuracy_dict[temp_size]['test_score'].append(temp_tree.score(temp_X_test, temp_y_test))\n",
    "\n",
    "accuracy_dict[train_size[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs\n",
    "plt.figure(figsize = (15, 15), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.suptitle(f'Accuracy as a function of train-test split\\n(N = 1317) | random_state: {universal_random_seed} | Stratified\\n')\n",
    "\n",
    "ax_count = 1\n",
    "for train_size in list(accuracy_dict.keys()):\n",
    "    temp_test_scores = accuracy_dict[train_size]['test_score']\n",
    "    temp_train_scores = accuracy_dict[train_size]['train_score']\n",
    "\n",
    "    # reshape scores, to only include non-repeated training scores (where nodes have all become pure)\n",
    "    temp_max_train_index = np.min( np.array(range(1,16))[temp_train_scores == np.max(temp_train_scores)] ) # this becomes final value\n",
    "    #temp_index = list(range(0, temp_max_train_index))\n",
    "\n",
    "    temp_test_scores = temp_test_scores[0:temp_max_train_index]\n",
    "    temp_train_scores = temp_train_scores[0:temp_max_train_index]\n",
    "    \n",
    "    plt.subplot(3, 3, ax_count)\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_train_scores, label = 'Training Accuracy', marker = 'o')\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_test_scores, label = 'Test Accuracy', marker = 'o')\n",
    "    \n",
    "    max_test_scores = np.array(range(1, temp_max_train_index+1))[temp_test_scores == np.max(temp_test_scores)]\n",
    "    max_score_count = 1\n",
    "    for max_test_score in max_test_scores:\n",
    "        if max_score_count == 1:\n",
    "            first_max_test_score = max_test_score\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--', label = 'Max Test Score')\n",
    "        else:\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--')\n",
    "        max_score_count += 1\n",
    "    temp_train_prop = int(round(train_size*100,0))\n",
    "    \n",
    "    plt.title(f'Train/Test Ratio: {temp_train_prop}:{100-temp_train_prop}.\\nMax Test score: {round(np.max(temp_test_scores),3)} | Average Test score: {round(np.mean(temp_test_scores),3)}\\nTest Score range: {round(np.max(temp_test_scores)-np.min(temp_test_scores),3)}\\nMax Test score depth: {first_max_test_score} | Max depth: {temp_max_train_index}')\n",
    "    if ax_count % 3 == 1:\n",
    "        plt.legend()\n",
    "    plt.ylim(bottom = 0.75, top = 1)\n",
    "    plt.xlim((0, 10))\n",
    "    plt.xticks([0, 5, 10, 15])\n",
    "    ax_count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_plots_trainProportion.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the decision trees look like for the final depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, train_size = .4, random_state = universal_random_seed, stratify = stratify_col)\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth = 1).fit(temp_X_train, temp_y_train)\n",
    "plot_tree(dtree, feature_names = ['smoking', 'age'], class_names = ['Alive', 'Dead'], filled = True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth = 2).fit(temp_X_train, temp_y_train)\n",
    "plot_tree(dtree, feature_names = ['smoking', 'age'], class_names = ['Alive', 'Dead'], filled = True)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtree = DecisionTreeClassifier(max_depth = 4).fit(temp_X_train, temp_y_train)\n",
    "#plot_tree(dtree, feature_names = ['smoking', 'age'], class_names = ['Alive', 'Dead'], filled = True)\n",
    "#plt.plot()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "plot_tree(dtree, feature_names = ['smoking', 'age'], class_names = ['Alive', 'Dead'], filled = True,\n",
    "              ax = ax, fontsize = 6)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that the range seemed to vary systematically when random_seed was 42, from previous runs. Re-doing above work here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = np.linspace(.1, .9, 9)\n",
    "max_depths = list(range(1, 16)) # iterate over max depth\n",
    "\n",
    "accuracy_dict = {}\n",
    "\n",
    "for temp_size in train_size:\n",
    "    accuracy_dict[temp_size] = {'train_score': [], 'test_score': []} # this time, instantiate train_score and test_score as empty lists onto which I will append accuracy values\n",
    "    temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, train_size = temp_size, random_state = 42, stratify = stratify_col)\n",
    "    for temp_depth in max_depths:\n",
    "        temp_tree = DecisionTreeClassifier(max_depth = temp_depth).fit(temp_X_train, temp_y_train)\n",
    "        \n",
    "        accuracy_dict[temp_size]['train_score'].append(temp_tree.score(temp_X_train, temp_y_train))\n",
    "        accuracy_dict[temp_size]['test_score'].append(temp_tree.score(temp_X_test, temp_y_test))\n",
    "\n",
    "# create graphs with random state 42\n",
    "plt.figure(figsize = (15, 15), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.suptitle('Accuracy as a function of train-test split\\n(N = 1317) | random_state: 42\\n')\n",
    "\n",
    "ax_count = 1\n",
    "for train_size in list(accuracy_dict.keys()):\n",
    "    temp_test_scores = accuracy_dict[train_size]['test_score']\n",
    "    temp_train_scores = accuracy_dict[train_size]['train_score']\n",
    "\n",
    "    # reshape scores, to only include non-repeated training scores (where nodes have all become pure)\n",
    "    temp_max_train_index = np.min( np.array(range(1,16))[temp_train_scores == np.max(temp_train_scores)] ) # this becomes final value\n",
    "    #temp_index = list(range(0, temp_max_train_index))\n",
    "\n",
    "    temp_test_scores = temp_test_scores[0:temp_max_train_index]\n",
    "    temp_train_scores = temp_train_scores[0:temp_max_train_index]\n",
    "    \n",
    "    plt.subplot(3, 3, ax_count)\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_train_scores, label = 'Training Accuracy', marker = 'o')\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_test_scores, label = 'Test Accuracy', marker = 'o')\n",
    "    \n",
    "    max_test_scores = np.array(range(1, temp_max_train_index+1))[temp_test_scores == np.max(temp_test_scores)]\n",
    "    max_score_count = 1\n",
    "    for max_test_score in max_test_scores:\n",
    "        if max_score_count == 1:\n",
    "            first_max_test_score = max_test_score\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--', label = 'Max Test Score')\n",
    "        else:\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--')\n",
    "        max_score_count += 1\n",
    "    temp_train_prop = int(round(train_size*100,0))\n",
    "    \n",
    "    plt.title(f'Train/Test Ratio: {temp_train_prop}:{100-temp_train_prop}.\\nMax Test score: {round(np.max(temp_test_scores),3)} | Average Test score: {round(np.mean(temp_test_scores),3)}\\nTest Score range: {round(np.max(temp_test_scores)-np.min(temp_test_scores),3)}\\nMax Test score depth: {first_max_test_score}')\n",
    "    if ax_count % 3 == 1:\n",
    "        plt.legend()\n",
    "    plt.ylim(bottom = 0.75, top = 1)\n",
    "    plt.xlim((1, 10))\n",
    "    plt.xticks([5, 10, 15])\n",
    "    ax_count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('accuracy_plots_trainProportion_42.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that there's a quadratic relationship b/t test score range and max depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What about an unstratified version of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = np.linspace(.1, .9, 9)\n",
    "max_depths = list(range(1, 16)) # iterate over max depth\n",
    "\n",
    "accuracy_dict = {}\n",
    "\n",
    "for temp_size in train_size:\n",
    "    accuracy_dict[temp_size] = {'train_score': [], 'test_score': []} # this time, instantiate train_score and test_score as empty lists onto which I will append accuracy values\n",
    "    temp_X_train, temp_X_test, temp_y_train, temp_y_test = train_test_split(X, y, train_size = temp_size, random_state = universal_random_seed)\n",
    "    for temp_depth in max_depths:\n",
    "        temp_tree = DecisionTreeClassifier(max_depth = temp_depth).fit(temp_X_train, temp_y_train)\n",
    "        \n",
    "        accuracy_dict[temp_size]['train_score'].append(temp_tree.score(temp_X_train, temp_y_train))\n",
    "        accuracy_dict[temp_size]['test_score'].append(temp_tree.score(temp_X_test, temp_y_test))\n",
    "\n",
    "# create graphs with random state 42\n",
    "plt.figure(figsize = (15, 15), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.suptitle(f'Accuracy as a function of train-test split\\n(N = 1317) | random_state: {universal_random_seed} | Unstratified\\n')\n",
    "\n",
    "ax_count = 1\n",
    "for train_size in list(accuracy_dict.keys()):\n",
    "    temp_test_scores = accuracy_dict[train_size]['test_score']\n",
    "    temp_train_scores = accuracy_dict[train_size]['train_score']\n",
    "\n",
    "    # reshape scores, to only include non-repeated training scores (where nodes have all become pure)\n",
    "    temp_max_train_index = np.min( np.array(range(1,16))[temp_train_scores == np.max(temp_train_scores)] ) # this becomes final value\n",
    "    #temp_index = list(range(0, temp_max_train_index))\n",
    "\n",
    "    temp_test_scores = temp_test_scores[0:temp_max_train_index]\n",
    "    temp_train_scores = temp_train_scores[0:temp_max_train_index]\n",
    "    \n",
    "    plt.subplot(3, 3, ax_count)\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_train_scores, label = 'Training Accuracy', marker = 'o')\n",
    "    plt.plot(list(range(1, temp_max_train_index+1)), temp_test_scores, label = 'Test Accuracy', marker = 'o')\n",
    "    \n",
    "    max_test_scores = np.array(range(1, temp_max_train_index+1))[temp_test_scores == np.max(temp_test_scores)]\n",
    "    max_score_count = 1\n",
    "    for max_test_score in max_test_scores:\n",
    "        if max_score_count == 1:\n",
    "            first_max_test_score = max_test_score\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--', label = 'Max Test Score')\n",
    "        else:\n",
    "            plt.axvline(x = max_test_score, color = 'red', linestyle = '--')\n",
    "        max_score_count += 1\n",
    "    temp_train_prop = int(round(train_size*100,0))\n",
    "    \n",
    "    plt.title(f'Train/Test Ratio: {temp_train_prop}:{100-temp_train_prop}.\\nMax Test score: {round(np.max(temp_test_scores),3)} | Average Test score: {round(np.mean(temp_test_scores),3)}\\nTest Score range: {round(np.max(temp_test_scores)-np.min(temp_test_scores),3)}\\nMax Test score depth: {first_max_test_score}')\n",
    "    if ax_count % 3 == 1:\n",
    "        plt.legend()\n",
    "    plt.ylim(bottom = 0.75, top = 1)\n",
    "    plt.xlim((1, 10))\n",
    "    plt.xticks([5, 10, 15])\n",
    "    ax_count += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_plots_trainProportion_unstratified.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
